# ğŸš€ Mini vLLM â€“ CPU LLM Inference Server

A production-style Large Language Model (LLM) inference backend built from scratch using **llama.cpp**, **FastAPI**, and pure systems thinking.

This project demonstrates how real-world inference infrastructure works â€” including KV cache behavior, thread tuning, quantization benchmarking, and live observability.

---

# ğŸ§  Project Goal

Build a CPU-based inference server that includes:

* Model loading (GGUF via llama.cpp)
* Chat completion API
* KV cache experimentation
* Thread performance tuning
* Quantization benchmarking
* Metrics endpoint
* Experimental cache complexity analysis

This is not an API wrapper.
This is an infrastructure-level implementation.

---

# ğŸ— Architecture Overview

```
User
  â†“
FastAPI (/generate)
  â†“
llama.cpp (GGUF model)
  â†“
KV Cache (attention reuse)
  â†“
Metrics Collector
  â†“
/metrics endpoint
```

Core components:

* `model.py` â†’ Model loader
* `main.py` â†’ FastAPI server
* `metrics.py` â†’ Observability layer
* `kv_cache.py` â†’ Cache experiment

---

# âš™ï¸ Tech Stack

* Python 3.12
* llama-cpp-python
* FastAPI
* Uvicorn
* psutil
* GGUF Quantized Models

---

# ğŸ§ª KV Cache Experiment

## Objective

Measure performance difference between:

* KV cache enabled (default)
* KV cache destroyed (forced full recomputation)

## Results (TinyLlama 1.1B Q4, CPU 4 cores)

### With KV Cache

* 50 tokens generated
* Total latency â‰ˆ **2.5 seconds**
* â‰ˆ **0.05 sec per token**
* â‰ˆ 26â€“28 tokens/sec

### Without KV Cache (Forced Reset)

* 50 tokens generated
* Total latency â‰ˆ **44.6 seconds**
* â‰ˆ **0.89 sec per token (avg)**
* ~18x slower overall

## Explanation

Without KV cache:

Each token recomputes attention over all previous tokens.

Total attention work:

```
1 + 2 + 3 + ... + N â‰ˆ O(NÂ²)
```

With KV cache:

Keys and Values are stored.
Each new token computes attention only once.

Total work:

```
N Ã— constant â‰ˆ O(N)
```

KV cache converts quadratic growth into linear incremental cost.

This is the reason modern chat systems are usable.

---

# ğŸ§µ Thread Tuning Experiment

Machine: 4 logical cores (`nproc = 4`)

| Threads | Tokens/sec |
| ------- | ---------- |
| 2       | 28.6       |
| 3       | 27.0       |
| 4       | 25.8       |

Optimal configuration: **n_threads = 2**

Observation:

Increasing threads did not improve performance.
Inference was memory-bandwidth bound, not compute-bound.

---

# ğŸ“Š Metrics Endpoint

Expose live server statistics:

```
GET /metrics
```

Returns:

```json
{
  "total_requests": 2,
  "avg_latency_sec": 6.86,
  "tokens_per_sec_avg": 26.6,
  "total_tokens_generated": 365,
  "memory_mb": 1213.11,
  "cpu_percent": 21.1
}
```

Provides:

* Request count
* Average latency
* Aggregate throughput
* RAM usage
* CPU usage

Observability is built-in.

---

# ğŸ§  Quantization Insight

Model tested: TinyLlama 1.1B Q4_K_M

Memory footprint â‰ˆ 1.2GB (including buffers and cache)

Quantization reduces:

* RAM usage
* Memory bandwidth
* Latency per token

Tradeoff: slight reduction in output quality.

---

# ğŸš€ How To Run

## 1ï¸âƒ£ Install dependencies

```
pip install llama-cpp-python fastapi uvicorn psutil
```

## 2ï¸âƒ£ Start server

```
python -m uvicorn server.main:app --reload
```

## 3ï¸âƒ£ Generate text

```
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Explain gravity simply"}'
```

## 4ï¸âƒ£ View metrics

```
curl http://localhost:8000/metrics
```

---

# ğŸ“ˆ Key Learnings

* KV cache is mandatory for usable LLM inference
* CPU inference is often memory-bandwidth bound
* More threads â‰  better performance
* Observability is essential before optimization
* Quantization dramatically affects throughput

---

# ğŸ¯ What This Project Demonstrates

This project shows understanding of:

* Transformer attention mechanics
* KV caching behavior
* Systems performance profiling
* Async API serving
* CPU-level optimization
* Infrastructure thinking

---

# ğŸ”® Next Steps

* Add batching scheduler
* Implement streaming tokens (SSE)
* Add quantization comparison benchmarks
* Dockerize deployment
* Publish performance graphs

---

# ğŸ“Œ Conclusion

This Mini vLLM implementation proves that efficient inference is not about model size â€” it is about systems design.

KV cache turns quadratic complexity into linear growth.
Thread tuning reveals memory bottlenecks.
Metrics make performance measurable.

Inference is infrastructure.
And infrastructure is engineering.
